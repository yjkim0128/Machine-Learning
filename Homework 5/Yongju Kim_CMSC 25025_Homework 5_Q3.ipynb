{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UOLsXd5yvl99"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uXpGnDfJdOpX"
   },
   "source": [
    "Discussed this assignment with Hyun In Park, Heeseung Hwang, and Sang Hoon Kim.  \n",
    "This notebook was originally written and executed on Google colab, then opened via Jupyter for PDF translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wskAFLuUbgeC"
   },
   "source": [
    "**Preliminary setup:**  \n",
    "\n",
    "As the task is overall highly memory-consuming and often crashes the session, I serialized key data at each checkpoint. If the session gets killed, I did not re-execute entirety of the code but simply loaded stored pickle files and continued on the leftover parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2nkAoWu0eB-9"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "whQmAuSNuPha"
   },
   "outputs": [],
   "source": [
    "with open(\"/content/drive/My Drive/cmsc25025/wiki-text.txt\", \"r\") as f:\n",
    "  wikidata = f.read().replace(\"\\n\", \" \")\n",
    "  f.close()\n",
    "\n",
    "wikidata = wikidata.lower().split()[:25000000]\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "cnt_wikidata = Counter(wikidata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3rbxuQGbvobb"
   },
   "source": [
    "After filtering out stop_words and those appearing 100 times or less, I got the final vocabulary set with size of 14960.  \n",
    "The corpus and vocabulary size (i.e. filtering threshold) are determined with the target of (i) having approx. 15000 words in the vocabulary set, and (ii) keep the corpus and vocabulary to the manageable size so that the session won't crash (often, at least)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IiL1th6tN387"
   },
   "outputs": [],
   "source": [
    "wikidata = list(filter(lambda x: x not in stop_words, wikidata))\n",
    "wikidata = list(filter(lambda x: cnt_wikidata[x] > 100, wikidata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZYCdd4MNy76"
   },
   "outputs": [],
   "source": [
    "vocabulary = set(wikidata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "icmvvkAltck_"
   },
   "outputs": [],
   "source": [
    "with open(r\"/content/drive/My Drive/cmsc25025/saved_wikidata.p\", \"wb\") as output_file:\n",
    "  pickle.dump(wikidata, output_file)\n",
    "  output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b84tEC49in6n"
   },
   "outputs": [],
   "source": [
    "with open(r\"/content/drive/My Drive/cmsc25025/saved_vocab.p\", \"wb\") as output_file:\n",
    "  pickle.dump(vocabulary, output_file)\n",
    "  output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fa_GxD7huS5g"
   },
   "outputs": [],
   "source": [
    "with open(r\"/content/drive/My Drive/cmsc25025/saved_wikidata.p\", \"rb\") as input_file:\n",
    "  wikidata = pickle.load(input_file)\n",
    "  input_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZXlXhWujudg7"
   },
   "outputs": [],
   "source": [
    "with open(r\"/content/drive/My Drive/cmsc25025/saved_vocab.p\", \"rb\") as input_file:\n",
    "  vocabulary = pickle.load(input_file)\n",
    "  input_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tP9IqHhoKnm8",
    "outputId": "a333ef75-7cd1-4756-a4f1-c1123babe366"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14960"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "USQZCNsxbmtD"
   },
   "source": [
    "**Part (a):**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HrKkd_Tj_78q"
   },
   "outputs": [],
   "source": [
    "#Computing positive samples\n",
    "Sp_pair = []\n",
    "\n",
    "for i in range(len(wikidata)):\n",
    "\n",
    "  center = wikidata[i]\n",
    "  context = wikidata[i-5:i] + wikidata[i+1:i+6]\n",
    "\n",
    "  Sp_pair += [(center, word) for word in context]\n",
    "\n",
    "with open(r\"/content/drive/My Drive/cmsc25025/saved_sppair_all.p\", \"wb\") as output_file:\n",
    "  pickle.dump(Sp_pair, output_file)\n",
    "  output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jY8BLBRnGlse"
   },
   "outputs": [],
   "source": [
    "#Saving positive samples\n",
    "with open(r\"/content/drive/My Drive/cmsc25025/saved_sppair_all.p\", \"rb\") as input_file:\n",
    "  Sp_pair = pickle.load(input_file)\n",
    "  input_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jtgxiJneHaa2"
   },
   "outputs": [],
   "source": [
    "spraw0 = [x[0] for x in Sp_pair]\n",
    "spraw1 = [x[1] for x in Sp_pair]\n",
    "Sp_raw = spraw0 + spraw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rSNo9rtZH0Mz"
   },
   "outputs": [],
   "source": [
    "cnt_Sp_pair = Counter(Sp_pair)\n",
    "cnt_Sp_raw = Counter(Sp_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tDyzjgXmH-4Q"
   },
   "outputs": [],
   "source": [
    "def compute_pmi_row(i):\n",
    "  '''\n",
    "  Computes ith row of M\n",
    "  '''\n",
    "  wi = vocablist[i]\n",
    "  N_wi = cnt_Sp_raw[wi]\n",
    "\n",
    "  #treating the positive sample set with the unordered-ness/symmetricity of (w,c) in consideration\n",
    "  row_numer = np.array([cnt_Sp_pair[(wi, wj)]+cnt_Sp_pair[(wj, wi)]+1 for wj in vocablist])\n",
    "  row_denom = np.array([cnt_Sp_raw[wj] for wj in vocablist])\n",
    "  row = row_numer / row_denom\n",
    "  row = np.log(row * N_Sp / N_wi)\n",
    "\n",
    "  return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4oQC2kV-IhFf"
   },
   "outputs": [],
   "source": [
    "M = np.zeros((len(vocabulary), len(vocabulary)))\n",
    "vocablist = list(vocabulary)\n",
    "N_Sp = len(Sp_pair)\n",
    "\n",
    "for i in range(len(vocabulary)):\n",
    "  \n",
    "  M[i, :] = compute_pmi_row(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sf3xNum6AKVJ"
   },
   "source": [
    "**Part (b):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h02lgrsawVx-"
   },
   "outputs": [],
   "source": [
    "#SVD\n",
    "U, Sigma, Vt = svds(csr_matrix(M), k = 50)\n",
    "Sigma = np.diag(Sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S9kbEwg4Aee5"
   },
   "source": [
    "**Part (c):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BgLjBKlqAf-t"
   },
   "outputs": [],
   "source": [
    "W = np.matmul(U, Sigma ** 0.5) #embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zGZEibPSL7HV"
   },
   "outputs": [],
   "source": [
    "# Saves embedding matrix\n",
    "with open(r\"/content/drive/My Drive/cmsc25025/saved_embedding.p\", \"wb\") as output_file:\n",
    "  pickle.dump(W, output_file)\n",
    "  output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jv7trDbXBGH1"
   },
   "source": [
    "**Part (d):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "co3Y2-xiBKHS"
   },
   "outputs": [],
   "source": [
    "def get_n_closest(word, n, embedding, already_exists = True):\n",
    "  '''\n",
    "  word should be a string if it already exists = True, and\n",
    "  a vector otherwise\n",
    "  '''\n",
    "  if already_exists:\n",
    "    ind = vocablist.index(word)\n",
    "    embedding = embedding - embedding[ind]\n",
    "    distance = np.apply_along_axis(np.linalg.norm, -1, embedding)\n",
    "    closest_ind = list(np.argsort(distance)[1:n+1])\n",
    "  \n",
    "  else:\n",
    "    embedding = embedding - word\n",
    "    distance = np.apply_along_axis(np.linalg.norm, -1, embedding)\n",
    "    closest_ind = list(np.argsort(distance))[:n]\n",
    "\n",
    "  return closest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "dRTeA4j6E8f1",
    "outputId": "64005a20-9885-4156-fcb3-7cc02d29495b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "physics is most similar to: mechanics, quantum, einstein, relativity, fields\n",
      "republican is most similar to: democrats, presidential, presidency, electoral, senator\n",
      "einstein is most similar to: relativity, newton, experiment, maxwell, physicists\n",
      "algebra is most similar to: algebraic, finite, theorem, calculus, spaces\n",
      "fish is most similar to: fruit, wild, meat, trees, plant\n"
     ]
    }
   ],
   "source": [
    "examples = {\"physics\": [], \"republican\": [], \"einstein\": [], \"algebra\": [], \"fish\": []}\n",
    "\n",
    "for word in examples:\n",
    "\n",
    "  closest = get_n_closest(word, 5, W)\n",
    "\n",
    "  for ind in closest:\n",
    "    examples[word].append(vocablist[ind])\n",
    "  \n",
    "for word in examples:\n",
    "  print(\"{:s} is most similar to: {:s}, {:s}, {:s}, {:s}, {:s}\".format(\n",
    "      word, examples[word][0], examples[word][1], examples[word][2], \n",
    "      examples[word][3], examples[word][4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tl7HYhpuR8z1"
   },
   "source": [
    "For physics, republican, einstein, and algebra, all top 5 similar words appear to be very relevant to the respective vocabulary.  \n",
    "For example, \"quantum mechanics\" is a field of \"physics,\" \"relativity\" is a research achievement by \"einstein,\" who is a famous \"physicist(s)\" just like \"newton\" or \"maxwell.\" \"Repbulican\" will compete with \"democrats\" for the \"presidency,\" and \"algebra\" will involve \"theorems\" about (say, vector) \"spaces.\"  \n",
    "On the other hand, similar words to \"fish\" seem to be selected from a broader domain. While all of 5 words have connections to \"fish\" in some sense, \"fruit\" and \"meat\" appear to consider the food-aspect of fish, while \"wild,\" \"trees,\" and \"plant\" consider the animal-aspect of fish (within the broader scope of biology/ecology/nature, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ldEwXMfZDgj8"
   },
   "source": [
    "**Part (e):**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mTgVsDI0Ds5P"
   },
   "source": [
    "3 analogies I used are:\n",
    "\n",
    "einstein:scientist = clinton: ?  \n",
    "left:right = democrat: ?    \n",
    "love:hate = peace: ?  \n",
    "\n",
    "The natural answers I expect by common sense are: \"president,\" \"republican,\" and \"war.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o21AqAYjDsNH"
   },
   "outputs": [],
   "source": [
    "analogies = [(\"einstein\", \"scientist\", \"clinton\"), (\"left\", \"right\", \"democrat\"), (\"love\", \"hate\", \"peace\")]\n",
    "guess = []\n",
    "\n",
    "for case in analogies:\n",
    "\n",
    "  v1_ind, v2_ind, v3_ind = vocablist.index(case[0]), vocablist.index(case[1]), vocablist.index(case[2])\n",
    "  v = W[v2_ind, :] - W[v1_ind, :] + W[v3_ind, :]\n",
    "\n",
    "  guess_ind = get_n_closest(v, 1, W, False)\n",
    "  guess.append(vocablist[guess_ind[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "8bhdI1BZHDvp",
    "outputId": "6185e399-0021-4d93-ebe7-5c3aa98812d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analogy guess made by the algorithm\n",
      "\n",
      "einstein  :  scientist  =  clinton  :  kennedy\n",
      "left  :  right  =  democrat  :  amendment\n",
      "love  :  hate  =  peace  :  negotiations\n"
     ]
    }
   ],
   "source": [
    "print(\"Analogy guess made by the algorithm\\n\")\n",
    "\n",
    "for i in range(3):\n",
    "  #print(\"{:s} : {:s} = {:s} : {:s}\".format(analogies[i][0], analogies[i][1], analogies[i],[2], guess[i]))\n",
    "  print(analogies[i][0], \" : \", analogies[i][1], \" = \", analogies[i][2], \" : \", guess[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PzDwSwV1PGHz"
   },
   "source": [
    "Answers for all three analogies were different from my expectation, but those answers by the algorithm still seem to be grasping some sense of context / relationship between words.  \n",
    "For example, based on enstein-scientist (person-occupation) pair, the program guessed \"kennedy\"--another president (i.e. same occupation) from the same party--for \"clinton.\"  \n",
    "Based on the left-right analogy (antonyms), the software brought \"amendment\" for \"democrat.\" It is not \"republican\" as expected, but still in the context of politics. (I'm not sure if republicans like amendments more than democrats do, though.)  \n",
    "Finally, based on love-hate analogy (antonyms again), I expected \"war\" while the algorithm guessed \"negotiations\" for \"peace.\" I'd say negotiation is definitely relevant to war and thus the program is not really missing the context, but whether or not it is an antonym of \"peace\" could be controversial.  \n",
    "\n",
    "If it was given more data (at the cost of computing time) or the original corpus was pre-processed better (in terms of text processing, e.g. regex, etc.), the algorithm might have performed better on these analogies."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "CMSC 25025 Homework 5 Question 3 - Yongju Kim.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
